package main

import (
	"errors"
	"flag"
	"fmt"
	"io/fs"
	"net/http"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"time"

	"image_hub/model"
	"image_hub/spiders"

	"github.com/PuerkitoBio/goquery"
	nested "github.com/antonfisher/nested-logrus-formatter"
	"github.com/gocolly/colly/v2"
	"github.com/gocolly/colly/v2/queue"
	rotatelogs "github.com/lestrrat-go/file-rotatelogs"
	log "github.com/sirupsen/logrus"
	"github.com/spf13/viper"
)

var (
	MODE    string
	VERSION string

	serviceName       string
	configFile        string
	defaultConfigFile = "conf/config.yaml"

	// Define the directory to traverse
	dir = "D:/work/wechat_download_data/html/test"
)

func main() {

	if MODE == "" {
		MODE = "debug"
	}

	err := Init()
	if err != nil {
		fmt.Println(err)
		return
	}

	Run()
}

// åˆå§‹åŒ–
func Init() error {

	err := initFlag()
	if err != nil {
		return err
	}

	//init config
	err = initConfig(configFile)
	if err != nil {
		return errors.New("config init failed, err:" + err.Error())
	}

	serviceName = viper.GetString("server.server_name")
	err = initLog()
	if err != nil {
		return err
	}

	model.Init()
	return nil
}

func Run() {

	// request local files
	// https://github.com/gocolly/colly/blob/master/_examples/local_files/local_files.go
	t := &http.Transport{}
	t.RegisterProtocol("file", http.NewFileTransport(http.Dir("/")))

	// Collector
	// çˆ¬å–æœ¬åœ°æ–‡ä»¶æ—¶,ä¸ç”¨è®¾ç½®AllowedDomains

	c := colly.NewCollector(
		colly.AllowURLRevisit(),
	)

	c.WithTransport(t)

	// Limit the number of threads started by colly to two
	// when visiting links which domains' matches "*httpbin.*" glob
	c.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: 3,
		RandomDelay: 16 * time.Second,
	})

	// create a request queue with 2 consumer threads
	q, _ := queue.New(
		20, // Number of consumer threads
		&queue.InMemoryQueueStorage{MaxSize: 1000000}, // Use default queue storage
	)

	c.OnRequest(func(r *colly.Request) {

		fmt.Printf("=============== c.OnRequest. url: %v\n", r.URL.String())

		urlType := r.Ctx.Get(spiders.UrlTypeKey)

		isEmpty := q.IsEmpty()
		size, err := q.Size()
		if err != nil {
			log.Infof("Queue.Size() return an error: %v", err)
		}
		threads := q.Threads
		log.Infof("OnRequest: Req.ID: %d, urlType:%s, Req.URL: %s, q.IsEmpty: %+v, q.Size: %d, q.Threads: %d\n", r.ID, urlType, r.URL, isEmpty, size, threads)
	})

	c.OnResponse(func(r *colly.Response) {

		urlType := r.Ctx.Get(spiders.UrlTypeKey)
		isEmpty := q.IsEmpty()
		size, err := q.Size()
		if err != nil {
			log.Infof("Queue.Size() return an error: %v", err)
		}
		threads := q.Threads
		log.Infof("OnResponse: Req.ID: %d, urlType:%s, Req.URL: %s, Res.Body.len: %d bytes, q.IsEmpty: %+v, q.Size: %d, q.Threads: %d\n", r.Request.ID, urlType, r.Request.URL, len(r.Body), isEmpty, size, threads)
	})

	c.OnHTML("html", func(e *colly.HTMLElement) {

		urlType := e.Response.Ctx.Get(spiders.UrlTypeKey)
		log.Infof("OnHTML: [%d]%s, %s\n", e.Request.ID, urlType, e.Request.URL)

		switch urlType {

		// ç¬¬1æ¡å†…å®¹
		case spiders.FirstPage:
			firstPageSpider := spiders.NewFirstPage(spiders.FirstPage)
			err := firstPageSpider.Process(q, e, "")
			if err != nil {
				log.Errorf("firstPageSpider.Process failed. err: %s\n", err)
			}
			fmt.Println("c.OnHTML ç¬¬1æ¡å†…å®¹, é€€å‡º")

		// ç¬¬2æ¡å†…å®¹
		case spiders.SecondPage:
			secondPageSpider := spiders.NewSecondPage(spiders.SecondPage)
			err := secondPageSpider.Process(q, e, "")
			if err != nil {
				log.Errorf("secondPageSpider.Process failed. err: %s\n", err)
			}

		// ç¬¬3æ¡å†…å®¹
		case spiders.ThirdPage:
			thirdPageSpider := spiders.NewThirdPage(spiders.ThirdPage)
			err := thirdPageSpider.Process(q, e, "")
			if err != nil {
				log.Errorf("thirdPageSpider.Process failed. err: %s\n", err)
			}

		// ç¬¬4æ¡å†…å®¹
		case spiders.FourPage:
			fourPageSpider := spiders.NewFourPage(spiders.FourPage)
			err := fourPageSpider.Process(q, e, "")
			if err != nil {
				log.Errorf("fourPageSpider.Process failed. err: %s\n", err)
			}
		}
	})

	// OnScrapedä¸­è·å–çš„urlTypeå‚æ•°é”™è¯¯,å…ˆå¿½ç•¥
	c.OnScraped(func(r *colly.Response) {

		// urlType := r.Ctx.Get(UrlTypeKey)
		// log.Infof("OnScraped: [%d]%s,%s\n", r.Request.ID, urlType, r.Request.URL)
	})

	c.OnError(func(r *colly.Response, err error) {

		urlType := r.Ctx.Get(spiders.UrlTypeKey)
		fmt.Printf("OnError: [%d]%s, %s, %v\n", r.Request.ID, urlType, r.Request.URL, err)
		log.Infof("OnError: [%d]%s, %s, %v\n", r.Request.ID, urlType, r.Request.URL, err)
	})

	// Determine which spider to use based on the file count and file name
	var spider spiders.Spider

	// éå†ç›®å½•D:\work\wechat_download_data\html\Dump-0421-11-15-39ä¸‹çš„æ‰€æœ‰htmlæ–‡ä»¶
	// htmlæ–‡ä»¶åè§„åˆ™ä¸ºï¼š"%Y%m%d_%H%M%S"_"åºå·.html", ä¾‹å¦‚: 20230109_111900_1.html
	// åºå·ä¸º1æ—¶ï¼Œä½¿ç”¨firstPageSpiderè§£æ
	// åºå·ä¸º2æ—¶ï¼Œä½¿ç”¨secondPageSpiderè§£æ
	// åºå·ä¸º3æ—¶ï¼Œä½¿ç”¨thirdPageSpiderè§£æ
	// åºå·ä¸º4æ—¶ï¼Œä½¿ç”¨fourPageSpiderè§£æ
	// é€šè¿‡åˆ¤æ–­é¡µé¢å†…å›¾ç‰‡æ ‡ç­¾æ•°é‡å’Œé¡µé¢ç´¢å¼•æ¥å†³å®šä½¿ç”¨çš„å†…å®¹åŒ¹é…è§„åˆ™
	// åŒ¹é…è§„åˆ™ä¸€èˆ¬æ˜¯ï¼šæŒ‰ä»€ä¹ˆé¡ºåºï¼Œå–æ–‡å­—ï¼Œå–å›¾ç‰‡ï¼Œç„¶åç»„è£…ä¸ºä¸€ä¸ªå‘å¸ƒå†…å®¹ï¼Œå‘å¸ƒè‡³content_service
	// ç½‘é¡µçš„è§†è§‰ä¸Šçš„ä¸€ä¸ªåŒºå—(section) ç­‰äºcontent_serviceé‡Œé¢ä¸€ä¸ªå‘å¸ƒå†…å®¹(post)
	firstPageSpider := spiders.NewFirstPage(spiders.FirstPage)
	secondPageSpider := spiders.NewSecondPage(spiders.SecondPage)
	thirdPageSpider := spiders.NewThirdPage(spiders.ThirdPage)
	fourPageSpider := spiders.NewFourPage(spiders.FourPage)

	// Define the regular expression to match the file names
	re := regexp.MustCompile(`(\d{8}_\d{6})_(\d+)\.html`)

	// Traverse the directory and process each file
	err := filepath.WalkDir(dir, func(path string, d fs.DirEntry, err error) error {

		if err != nil {
			return err
		}
		if d.IsDir() {
			return nil
		}

		// Check if the file name matches the regular expression
		matches := re.FindStringSubmatch(d.Name())
		if len(matches) != 3 {
			return nil
		}

		// Extract the date and page number from the file name
		dateStr := matches[1]
		pageNumStr := matches[2]
		fmt.Printf("dateStr: %+v\n", dateStr)
		fmt.Printf("pageNumStr: %+v\n", pageNumStr)
		fmt.Printf("path: %+v\n", path)

		// æ‰“å¼€æœ¬åœ° HTML æ–‡ä»¶
		file, err := os.Open(path)
		if err != nil {
			return err
		}
		defer file.Close()

		// ä½¿ç”¨ goquery è§£æ HTML
		doc, err := goquery.NewDocumentFromReader(file)
		if err != nil {
			return err
		}

		selector := "meta[property='og:title']"
		title, isExist := doc.Find(selector).Attr("content")
		if isExist {

			if strings.Contains(title, "å¤´åƒ") {
				spider = firstPageSpider
			} else if strings.Contains(title, "èƒŒæ™¯å›¾") {
				spider = secondPageSpider
			} else if strings.Contains(title, "å£çº¸") {
				spider = thirdPageSpider
			} else if strings.Contains(title, "è¡¨æƒ…") || strings.Contains(title, "è¡¨æƒ…åŒ…") {
				spider = fourPageSpider
			} else {
				return fmt.Errorf("no matching spider found for file %s", d.Name())
			}

			fmt.Printf("title: %+v, spider: %+v\n", title, spider.GetName())

			// æ›¿æ¢ \ ä¸º /
			// D:\work\wechat_download_data\html\test\20220526_111900_1.html
			// D:/work/wechat_download_data/html/test/20220526_111900_1.html
			path = strings.ReplaceAll(path, "\\", "/")

			// Process the file with the selected spider
			err = spider.Process(q, nil, path)
			if err != nil {
				return err
			}
		} else {

			return fmt.Errorf("no matching content found for file %s", d.Name())
		}

		// dom, err := goquery.NewDocument(path)
		// if err != nil {
		// 	log.Fatalln(err)
		// }

		// dom.Find("p").Each(func(i int, selection *goquery.Selection) {
		// 	fmt.Println(selection.Text())
		// })

		// fileBytes, err := ioutil.ReadFile(path)
		// if err != nil {
		// 	return err
		// }

		// // æ ¹æ®é¡µé¢æ ‡é¢˜åˆ¤æ–­
		// // <meta property="og:title" content="ğ’ğ¡ğšğ«ğ&#39;&#39; æ‰‹æœºå£çº¸ | 4.23" />
		// fileContent := string(fileBytes)
		// titleRe := regexp.MustCompile(`<meta property="og:title" content="(.+?)"/>`)
		// titleMatches := titleRe.FindStringSubmatch(fileContent)
		// if len(titleMatches) != 2 {
		// 	return fmt.Errorf("no matching content found for file %s", d.Name())
		// }
		// title := titleMatches[1]

		return nil
	})

	// if err != nil {
	// 	log.Errorf("Error while processing files: %s\n", err)
	// }

	if err != nil {

		fmt.Printf("filepath.Walk err: %+v\n", err)
		panic(err)
	}
	q.Run(c)
}

// init flag
func initFlag() error {
	//init params
	h := flag.Bool("h", false, "application help")

	c := flag.String("c", "", "config file, ex: /data/config.yaml")

	flag.Parse()

	if *h {
		flag.PrintDefaults()
		os.Exit(0)
		return nil
	}

	if *c == "" {
		//use default config file
		path, _ := os.Executable()
		binPath := filepath.Dir(path)
		*c = binPath + "/../" + defaultConfigFile
	}

	configFile = *c

	return nil
}

func initConfig(configFile string) error {
	path, _ := os.Executable()
	RootPath := filepath.Dir(path)

	viper.Set("path.root", RootPath)

	configPath := filepath.Dir(configFile)
	fileName := filepath.Base(configFile)

	//è®¾ç½®è¯»å–çš„é…ç½®æ–‡ä»¶
	viper.SetConfigName(fileName)
	//æ·»åŠ è¯»å–çš„é…ç½®æ–‡ä»¶è·¯å¾„
	viper.AddConfigPath(configPath)
	//è®¾ç½®é…ç½®æ–‡ä»¶ç±»å‹
	viper.SetConfigType("yaml")

	if err := viper.ReadInConfig(); err != nil {
		return err
	}

	return nil
}

func initLog() error {

	logPathPrefix := fmt.Sprintf("%s/%s", viper.GetString("path.root"), viper.GetString("log.prefix"))
	logName := viper.GetString("log.name")
	logLevel := viper.GetInt("log.level")
	maxAge := viper.GetInt("log.maxAge")
	path := fmt.Sprintf("%s%s", logPathPrefix, logName)

	/* æ—¥å¿—è½®è½¬ç›¸å…³å‡½æ•°
	`WithLinkName` ä¸ºæœ€æ–°çš„æ—¥å¿—å»ºç«‹è½¯è¿æ¥
	`WithRotationTime` è®¾ç½®æ—¥å¿—åˆ†å‰²çš„æ—¶é—´ï¼Œéš”å¤šä¹…åˆ†å‰²ä¸€æ¬¡
	`WithMaxAge å’Œ WithRotationCountäºŒè€…åªèƒ½è®¾ç½®ä¸€ä¸ª
	`WithMaxAge` è®¾ç½®æ–‡ä»¶æ¸…ç†å‰çš„æœ€é•¿ä¿å­˜æ—¶é—´
	`WithRotationCount` è®¾ç½®æ–‡ä»¶æ¸…ç†å‰æœ€å¤šä¿å­˜çš„ä¸ªæ•°
	WithMaxAge WithRotationCount åªèƒ½å­˜åœ¨ä¸€ä¸ª
	*/
	// ä¸‹é¢é…ç½®æ—¥å¿—æ¯éš” 1 åˆ†é’Ÿè½®è½¬ä¸€ä¸ªæ–°æ–‡ä»¶ï¼Œä¿ç•™æœ€è¿‘ 3 åˆ†é’Ÿçš„æ—¥å¿—æ–‡ä»¶ï¼Œå¤šä½™çš„è‡ªåŠ¨æ¸…ç†æ‰ã€‚
	writer, err := rotatelogs.New(
		path+".%Y%m%d",
		rotatelogs.WithLinkName(path),
		rotatelogs.WithRotationTime(time.Duration(86400)*time.Second),
		rotatelogs.WithMaxAge(time.Duration(maxAge*24)*time.Hour),
	)
	if err != nil {
		return errors.New("initLog error:" + err.Error())
	}
	log.SetReportCaller(true)
	log.SetFormatter(&nested.Formatter{
		HideKeys:         true,
		NoUppercaseLevel: true,
		ShowFullLevel:    true,
		TimestampFormat:  "2006/01/02 15:04:05",
		NoFieldsSpace:    true,
		NoFieldsColors:   true,
		NoColors:         true,
		CallerFirst:      true,
		TrimMessages:     true,
		FieldsOrder:      []string{"component", "category"},
	})
	log.SetLevel(log.Level(logLevel))
	log.SetOutput(writer)
	return nil
}
